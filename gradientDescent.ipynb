{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aaman Bhandari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlgorithm GradientDescent(A, b, learning_rate, max_iterations):\\n1. Initialize x to some starting value (e.g., zero vector)\\n2. For i = 1 to max_iterations:\\n    2.1 Compute the gradient: gradient = A^T * (A * x - b)\\n    2.2 Update x: x = x - learning_rate * gradient\\n3. Return x\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Algorithm GradientDescent(A, b, learning_rate, max_iterations):\n",
    "1. Initialize x to some starting value (e.g., zero vector)\n",
    "2. For i = 1 to max_iterations:\n",
    "    2.1 Compute the gradient: gradient = A^T * (A * x - b)\n",
    "    2.2 Update x: x = x - learning_rate * gradient\n",
    "3. Return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(A, b, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization to minimize f(x) = 0.5 * ||A * x - b||^2.\n",
    "\n",
    "    Parameters:\n",
    "    - A: Matrix\n",
    "    - b: Vector\n",
    "    - learning_rate: Step size for the gradient descent\n",
    "    - max_iterations: Maximum number of iterations\n",
    "    - tolerance: Convergence criterion based on the norm of the gradient\n",
    "\n",
    "    Returns:\n",
    "    - x: Optimized vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize x to zero vector\n",
    "    x = np.zeros(A.shape[1])\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Compute the gradient\n",
    "        gradient = A.T @ (A @ x - b)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x - learning_rate * gradient\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01720523 0.48641745]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "b = np.array([1, 2, 3])\n",
    "optimized_x = gradient_descent(A, b)\n",
    "print(optimized_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
